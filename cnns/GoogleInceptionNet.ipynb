{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# 产生截断正态分布的函数\n",
    "trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 生成常用函数的默认参数\n",
    "def inception_v3_arg_scope(weight_decay=0.00004,\n",
    "                           stddev=0.1,\n",
    "                           batch_norm_var_collection='moving_vars'):\n",
    "\n",
    "    batch_norm_params = {\n",
    "        'decay': 0.9997,\n",
    "        'epsilon': 0.001,\n",
    "        'updates_collections': tf.GraphKeys.UPDATE_OPS,\n",
    "        'variables_collections': {\n",
    "            'beta': None,\n",
    "            'gamma': None,\n",
    "            'moving_mean': [batch_norm_var_collection],\n",
    "            'moving_variance': [batch_norm_var_collection],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 给函数的参数自动赋予某些默认值\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                  weights_regularizer=slim.l2_regularizer(weight_decay)):\n",
    "        with slim.arg_scope(\n",
    "                [slim.conv2d],\n",
    "                weights_initializer=trunc_normal(stddev),\n",
    "                activation_fn=tf.nn.relu,\n",
    "                normalizer_fn=slim.batch_norm,\n",
    "                normalizer_params=batch_norm_params) as sc:\n",
    "            return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 生成Inception V3网络的卷积部分\n",
    "def inception_v3_base(inputs, scope=None):\n",
    "\n",
    "    end_points = {}\n",
    "\n",
    "    with tf.variable_scope(scope, 'InceptionV3', [inputs]):\n",
    "        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
    "                        stride=1, padding='VALID'):\n",
    "            # 299 x 299 x 3\n",
    "            net = slim.conv2d(inputs, 32, [3, 3], stride=2, scope='Conv2d_1a_3x3')\n",
    "            # 149 x 149 x 32\n",
    "            net = slim.conv2d(net, 32, [3, 3], scope='Conv2d_2a_3x3')\n",
    "            # 147 x 147 x 32\n",
    "            net = slim.conv2d(net, 64, [3, 3], padding='SAME', scope='Conv2d_2b_3x3')\n",
    "            # 147 x 147 x 64\n",
    "            net = slim.max_pool2d(net, [3, 3], stride=2, scope='MaxPool_3a_3x3')\n",
    "            # 73 x 73 x 64\n",
    "            net = slim.conv2d(net, 80, [1, 1], scope='Conv2d_3b_1x1')\n",
    "            # 73 x 73 x 80.\n",
    "            net = slim.conv2d(net, 192, [3, 3], scope='Conv2d_4a_3x3')\n",
    "            # 71 x 71 x 192.\n",
    "            net = slim.max_pool2d(net, [3, 3], stride=2, scope='MaxPool_5a_3x3')\n",
    "            # 35 x 35 x 192.\n",
    "\n",
    "        # Inception blocks\n",
    "        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
    "                            stride=1, padding='SAME'):\n",
    "             # 第一个模块组\n",
    "            # 第1个模块: 35 x 35 x 256.\n",
    "            with tf.variable_scope('Mixed_5b'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv2d_0b_5x5')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "\n",
    "            # 第2个模块: 35 x 35 x 288.\n",
    "            with tf.variable_scope('Mixed_5c'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv_1_0c_5x5')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "\n",
    "            # 第3个模块: 35 x 35 x 288.\n",
    "            with tf.variable_scope('Mixed_5d'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv2d_0b_5x5')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "\n",
    "            # 第二个模块组\n",
    "            # 第1个模块: 17 x 17 x 768.\n",
    "            with tf.variable_scope('Mixed_6a'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 384, [3, 3], stride=2,\n",
    "                                         padding='VALID', scope='Conv2d_1a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_1 = slim.conv2d(branch_1, 96, [3, 3], stride=2,\n",
    "                                         padding='VALID', scope='Conv2d_1a_1x1')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',\n",
    "                                             scope='MaxPool_1a_3x3')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2], 3)\n",
    "\n",
    "            # 第2个模块: 17 x 17 x 768.\n",
    "            with tf.variable_scope('Mixed_6b'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 128, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 128, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 128, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                    branch_2 = slim.conv2d(branch_2, 128, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "\n",
    "            # 第3个模块: 17 x 17 x 768.\n",
    "            with tf.variable_scope('Mixed_6c'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 160, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "\n",
    "            # 第4个模块: 17 x 17 x 768.\n",
    "            with tf.variable_scope('Mixed_6d'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 160, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "\n",
    "            # 第5个模块: 17 x 17 x 768.\n",
    "            with tf.variable_scope('Mixed_6e'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            end_points['Mixed_6e'] = net\n",
    "\n",
    "            # 第三个模块组\n",
    "            # 第1个模块: 8 x 8 x 1280.\n",
    "            with tf.variable_scope('Mixed_7a'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_0 = slim.conv2d(branch_0, 320, [3, 3], stride=2,\n",
    "                                         padding='VALID', scope='Conv2d_1a_3x3')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [3, 3], stride=2,\n",
    "                                         padding='VALID', scope='Conv2d_1a_3x3')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',\n",
    "                                             scope='MaxPool_1a_3x3')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2], 3)\n",
    "\n",
    "            # 第2个模块: 8 x 8 x 2048.\n",
    "            with tf.variable_scope('Mixed_7b'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 320, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 384, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = tf.concat([\n",
    "                        slim.conv2d(branch_1, 384, [1, 3], scope='Conv2d_0b_1x3'),\n",
    "                        slim.conv2d(branch_1, 384, [3, 1], scope='Conv2d_0b_3x1')], 3)\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 448, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(\n",
    "                        branch_2, 384, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2 = tf.concat([\n",
    "                        slim.conv2d(branch_2, 384, [1, 3], scope='Conv2d_0c_1x3'),\n",
    "                        slim.conv2d(branch_2, 384, [3, 1], scope='Conv2d_0d_3x1')], 3)\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(\n",
    "                        branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "\n",
    "            # 第3个模块: 8 x 8 x 2048.\n",
    "            with tf.variable_scope('Mixed_7c'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 320, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 384, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = tf.concat([\n",
    "                        slim.conv2d(branch_1, 384, [1, 3], scope='Conv2d_0b_1x3'),\n",
    "                        slim.conv2d(branch_1, 384, [3, 1], scope='Conv2d_0c_3x1')], 3)\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 448, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(\n",
    "                        branch_2, 384, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2 = tf.concat([\n",
    "                        slim.conv2d(branch_2, 384, [1, 3], scope='Conv2d_0c_1x3'),\n",
    "                        slim.conv2d(branch_2, 384, [3, 1], scope='Conv2d_0d_3x1')], 3)\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(\n",
    "                        branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            \n",
    "            return net, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义全局平均池化、Softmax、Auxiliary Logits\n",
    "def inception_v3(inputs,\n",
    "                 num_classes=1000,   # 分类数量\n",
    "                 is_training=True,   # 标志是否是训练过程\n",
    "                 dropout_keep_prob=0.8,\n",
    "                 prediction_fn=slim.softmax,  # 分类函数\n",
    "                 spatial_squeeze=True,   # 标志是否对输出进行去除维数为1的维度\n",
    "                 reuse=None,    # 标志是否会对网络和Variable进行重复使用\n",
    "                 scope='InceptionV3'  # 包含函数默认参数的环境\n",
    "                ):\n",
    "    \n",
    "    # 定义网络的name和reuse等参数默认值\n",
    "    with tf.variable_scope(scope, 'InceptionV3', [inputs, num_classes], reuse=reuse) as scope:\n",
    "        # 定义Batch Normalization 和 Dropout的is_training标志默认值\n",
    "        with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):\n",
    "            # 构筑网络的卷积部分，得到最后一层的输出net和重要节点字典表end_points\n",
    "            net, end_points = inception_v3_base(inputs, scope=scope)\n",
    "\n",
    "            # Auxiliary Head logits辅助分类节点\n",
    "            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
    "                              stride=1, padding='SAME'):\n",
    "                aux_logits = end_points['Mixed_6e']\n",
    "                \n",
    "                with tf.variable_scope('AuxLogits'):\n",
    "                    aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3, \n",
    "                                                 padding='VALID', scope='AvgPool_1a_5x5')\n",
    "                    aux_logits = slim.conv2d(aux_logits, 128, [1, 1], scope='Conv2d_1b_1x1')\n",
    "\n",
    "                # Shape of feature map before the final layer.\n",
    "                aux_logits = slim.conv2d(aux_logits, 768, [5,5],\n",
    "                                        weights_initializer=trunc_normal(0.01),\n",
    "                                        padding='VALID', scope='Conv2d_2a_5x5')\n",
    "                aux_logits = slim.conv2d(aux_logits, num_classes, [1, 1], \n",
    "                                         activation_fn=None, normalizer_fn=None, \n",
    "                                         weights_initializer=trunc_normal(0.001),\n",
    "                                         scope='Conv2d_2b_1x1')\n",
    "                if spatial_squeeze:\n",
    "                    aux_logits = tf.squeeze(aux_logits, [1, 2], name='SpatialSqueeze')\n",
    "                end_points['AuxLogits'] = aux_logits\n",
    "\n",
    "            # Final pooling and prediction 分类预测逻辑\n",
    "            with tf.variable_scope('Logits'):\n",
    "                net = slim.avg_pool2d(net, [8, 8], padding='VALID', scope='AvgPool_1a_8x8')\n",
    "                # 1 x 1 x 2048\n",
    "                net = slim.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')\n",
    "                end_points['PreLogits'] = net\n",
    "                # 2048\n",
    "                logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n",
    "                                 normalizer_fn=None, scope='Conv2d_1c_1x1')\n",
    "                if spatial_squeeze:\n",
    "                    logits = tf.squeeze(logits, [1, 2], name='SpatialSqueeze')\n",
    "                # 1000\n",
    "\n",
    "            end_points['Logits'] = logits\n",
    "            end_points['Predictions'] = prediction_fn(logits, scope='Predictions')\n",
    "\n",
    "    return logits, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "def time_tensorflow_run(session, target, info_string):\n",
    "    num_steps_burn_in = 10\n",
    "    \n",
    "    total_duration = 0.0\n",
    "    total_duration_squared = 0.0\n",
    "    \n",
    "    for i in range(num_batches + num_steps_burn_in):\n",
    "        start_time = time.time()\n",
    "        _ = session.run(target)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        if i >= num_steps_burn_in:\n",
    "            if not i % 10:\n",
    "                print ('%s: step %d, duration = %.3f' %\n",
    "                       (datetime.now(), i - num_steps_burn_in, duration))\n",
    "                \n",
    "            total_duration += duration\n",
    "            total_duration_squared += duration * duration\n",
    "            \n",
    "    mn = total_duration / num_batches\n",
    "    vr = total_duration_squared / num_batches - mn * mn\n",
    "    sd = math.sqrt(vr)\n",
    "    \n",
    "    print ('%s: %s across %d steps, %.3f +/- %.3f sec / batch' %\n",
    "           (datetime.now(), info_string, num_batches, mn, sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-14 11:39:22.661718: step 0, duration = 0.472\n",
      "2017-11-14 11:39:27.392251: step 10, duration = 0.474\n",
      "2017-11-14 11:39:32.133838: step 20, duration = 0.473\n",
      "2017-11-14 11:39:36.858116: step 30, duration = 0.472\n",
      "2017-11-14 11:39:41.579713: step 40, duration = 0.472\n",
      "2017-11-14 11:39:46.304011: step 50, duration = 0.473\n",
      "2017-11-14 11:39:51.035114: step 60, duration = 0.472\n",
      "2017-11-14 11:39:55.771731: step 70, duration = 0.475\n",
      "2017-11-14 11:40:00.499119: step 80, duration = 0.473\n",
      "2017-11-14 11:40:05.228613: step 90, duration = 0.473\n",
      "2017-11-14 11:40:09.487835: Forward across 100 steps, 0.473 +/- 0.001 sec / batch\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "height, width = 299, 299\n",
    "\n",
    "inputs = tf.random_uniform((batch_size, height, width, 3))\n",
    "\n",
    "with slim.arg_scope(inception_v3_arg_scope()):\n",
    "    logits, end_points = inception_v3(inputs, is_training=False)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) \n",
    "\n",
    "num_batches=100\n",
    "time_tensorflow_run(sess, logits, \"Forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
