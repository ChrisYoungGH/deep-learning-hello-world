{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用CNN对cifar-10图片数据进行分类\n",
    "# cifar-10有10类图片，每类6000张，没有重叠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 载入numpy, time, 自动下载和读取CIFAR-10数据的类\n",
    "import cifar10_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义batch_size, 训练轮数max_steps, 数据默认路径\n",
    "max_steps = 3000\n",
    "batch_size = 128\n",
    "data_dir = '../models/tutorials/image/cifar10_data/cifar-10-batches-bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义初始化权重函数，使用L2 loss\n",
    "def variable_with_weight_loss(shape, stddev, wl):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    if wl is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var), wl, name='weight_loss') # 给weight加l2 loss\n",
    "        tf.add_to_collection('losses', weight_loss)  # 存储loss的collection\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "finish generating data\n"
     ]
    }
   ],
   "source": [
    "# 生成训练数据和测试数据\n",
    "# distorted函数中包含了一系列数据增强的操作，如随机水平翻转、随机剪切、设置随机亮度和对比度、标准化等\n",
    "images_train, labels_train = cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=batch_size)\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data=True, data_dir=data_dir, batch_size=batch_size)\n",
    "print('finish generating data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建输入数据的placeholder\n",
    "# batch_size在后面定义网络结构时被用到，所以不能用None了\n",
    "image_holder = tf.placeholder(tf.float32, [batch_size, 24, 24, 3])\n",
    "label_holder = tf.placeholder(tf.int32, [batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 第一个卷积层\n",
    "weight1 = variable_with_weight_loss(shape=[5,5,3,64], stddev=5e-2, wl=0.0) # 卷积核大小5x5,颜色通道3,卷积核数64,没有weight loss\n",
    "kernel1 = tf.nn.conv2d(image_holder, weight1, [1,1,1,1], padding='SAME')  # 步长为1\n",
    "bias1 = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "conv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))\n",
    "# 最大池化层尺寸和步长不一致可以增加数据的丰富性\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1], strides=[1,2,2,1], padding='SAME') # 尺寸3x3，步长2x2的最大池化层\n",
    "# LRN使响应比较大的值变得更大，抑制反馈小的神经元，对ReLU这种没有上限边界的激活函数有用\n",
    "norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)  # lrn层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 第二个卷积层，先LRN再最大池化\n",
    "weight2 = variable_with_weight_loss(shape=[5,5,64,64], stddev=5e-2, wl=0.0)\n",
    "kernel2 = tf.nn.conv2d(norm1, weight2, [1,1,1,1], padding='SAME')\n",
    "bias2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "conv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))\n",
    "norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 全连接层，使用L2正则约束，384个隐含节点\n",
    "reshape = tf.reshape(pool2, [batch_size, -1])\n",
    "dim = reshape.get_shape()[1].value\n",
    "weight3 = variable_with_weight_loss(shape=[dim,384], stddev=0.04, wl=0.004)\n",
    "bias3 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "local3 = tf.nn.relu(tf.matmul(reshape, weight3) + bias3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 全连接层，隐含节点数为前一个的一半，192个\n",
    "weight4 = variable_with_weight_loss(shape=[384,192], stddev=0.04, wl=0.004)\n",
    "bias4 = tf.Variable(tf.constant(0.1, shape=[192]))\n",
    "local4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 最后一层，模型Inference输出结果。\n",
    "weight5 = variable_with_weight_loss(shape=[192,10], stddev=1/192.0, wl=0.0)\n",
    "bias5 = tf.Variable(tf.constant(0.0, shape=[10]))\n",
    "logits = tf.add(tf.matmul(local4, weight5), bias5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义loss函数，将cross entropy计算的loss与weight l2 loss相加\n",
    "def loss(logits, labels):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # softmax和cross entropy一起计算\n",
    "        logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)  # 添加到整体loss\n",
    "    \n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss') # cross entropy loss和两个全连接层weight l2 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将logits节点和label_holder传入得到最终loss\n",
    "loss = loss(logits, label_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 优化器Adam\n",
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算top-k准确率，这里使用top 1，即输出分数最高的那一类的准确率\n",
    "top_k_op = tf.nn.in_top_k(logits, label_holder, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建默认session并初始化全部模型参数\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(Thread-4, started daemon 140106178483968)>,\n",
       " <Thread(Thread-5, started daemon 140106170091264)>,\n",
       " <Thread(Thread-6, started daemon 140106161698560)>,\n",
       " <Thread(Thread-7, started daemon 140106153305856)>,\n",
       " <Thread(Thread-8, started daemon 140106144913152)>,\n",
       " <Thread(Thread-9, started daemon 140105457071872)>,\n",
       " <Thread(Thread-10, started daemon 140105448679168)>,\n",
       " <Thread(Thread-11, started daemon 140105440286464)>,\n",
       " <Thread(Thread-12, started daemon 140105431893760)>,\n",
       " <Thread(Thread-13, started daemon 140105423501056)>,\n",
       " <Thread(Thread-14, started daemon 140105415108352)>,\n",
       " <Thread(Thread-15, started daemon 140105406715648)>,\n",
       " <Thread(Thread-16, started daemon 140104853092096)>,\n",
       " <Thread(Thread-17, started daemon 140104844699392)>,\n",
       " <Thread(Thread-18, started daemon 140104836306688)>,\n",
       " <Thread(Thread-19, started daemon 140104827913984)>,\n",
       " <Thread(Thread-20, started daemon 140104819521280)>,\n",
       " <Thread(Thread-21, started daemon 140104811128576)>,\n",
       " <Thread(Thread-22, started daemon 140104802735872)>,\n",
       " <Thread(Thread-23, started daemon 140104316221184)>,\n",
       " <Thread(Thread-24, started daemon 140104307828480)>,\n",
       " <Thread(Thread-25, started daemon 140104299435776)>,\n",
       " <Thread(Thread-26, started daemon 140104291043072)>,\n",
       " <Thread(Thread-27, started daemon 140104282650368)>,\n",
       " <Thread(Thread-28, started daemon 140104274257664)>,\n",
       " <Thread(Thread-29, started daemon 140104265864960)>,\n",
       " <Thread(Thread-30, started daemon 140103779350272)>,\n",
       " <Thread(Thread-31, started daemon 140103770957568)>,\n",
       " <Thread(Thread-32, started daemon 140103762564864)>,\n",
       " <Thread(Thread-33, started daemon 140103754172160)>,\n",
       " <Thread(Thread-34, started daemon 140103745779456)>,\n",
       " <Thread(Thread-35, started daemon 140103737386752)>,\n",
       " <Thread(Thread-36, started daemon 140103728994048)>,\n",
       " <Thread(Thread-37, started daemon 140103242479360)>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 启动线程队列\n",
    "tf.train.start_queue_runners()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss=4.68 (2.9 examples/sec; 43.941 sec/batch)\n",
      "step 10, loss=3.72 (428.3 examples/sec; 0.299 sec/batch)\n",
      "step 20, loss=3.09 (444.4 examples/sec; 0.288 sec/batch)\n",
      "step 30, loss=2.80 (431.8 examples/sec; 0.296 sec/batch)\n",
      "step 40, loss=2.42 (425.2 examples/sec; 0.301 sec/batch)\n",
      "step 50, loss=2.17 (425.9 examples/sec; 0.301 sec/batch)\n",
      "step 60, loss=2.09 (440.0 examples/sec; 0.291 sec/batch)\n",
      "step 70, loss=2.16 (452.5 examples/sec; 0.283 sec/batch)\n",
      "step 80, loss=1.99 (440.1 examples/sec; 0.291 sec/batch)\n",
      "step 90, loss=1.93 (476.1 examples/sec; 0.269 sec/batch)\n",
      "step 100, loss=1.99 (450.3 examples/sec; 0.284 sec/batch)\n",
      "step 110, loss=1.81 (437.8 examples/sec; 0.292 sec/batch)\n",
      "step 120, loss=1.89 (458.6 examples/sec; 0.279 sec/batch)\n",
      "step 130, loss=1.98 (437.3 examples/sec; 0.293 sec/batch)\n",
      "step 140, loss=1.84 (465.3 examples/sec; 0.275 sec/batch)\n",
      "step 150, loss=1.78 (422.2 examples/sec; 0.303 sec/batch)\n",
      "step 160, loss=1.78 (424.5 examples/sec; 0.301 sec/batch)\n",
      "step 170, loss=1.75 (462.0 examples/sec; 0.277 sec/batch)\n",
      "step 180, loss=1.67 (455.3 examples/sec; 0.281 sec/batch)\n",
      "step 190, loss=1.71 (441.8 examples/sec; 0.290 sec/batch)\n",
      "step 200, loss=1.65 (446.9 examples/sec; 0.286 sec/batch)\n",
      "step 210, loss=1.69 (486.6 examples/sec; 0.263 sec/batch)\n",
      "step 220, loss=1.66 (437.8 examples/sec; 0.292 sec/batch)\n",
      "step 230, loss=1.59 (456.8 examples/sec; 0.280 sec/batch)\n",
      "step 240, loss=1.58 (424.8 examples/sec; 0.301 sec/batch)\n",
      "step 250, loss=1.71 (428.9 examples/sec; 0.298 sec/batch)\n",
      "step 260, loss=1.69 (462.9 examples/sec; 0.277 sec/batch)\n",
      "step 270, loss=1.52 (430.1 examples/sec; 0.298 sec/batch)\n",
      "step 280, loss=1.73 (413.6 examples/sec; 0.309 sec/batch)\n",
      "step 290, loss=1.58 (445.5 examples/sec; 0.287 sec/batch)\n",
      "step 300, loss=1.55 (451.0 examples/sec; 0.284 sec/batch)\n",
      "step 310, loss=1.60 (441.0 examples/sec; 0.290 sec/batch)\n",
      "step 320, loss=1.66 (416.7 examples/sec; 0.307 sec/batch)\n",
      "step 330, loss=1.60 (419.0 examples/sec; 0.306 sec/batch)\n",
      "step 340, loss=1.50 (413.3 examples/sec; 0.310 sec/batch)\n",
      "step 350, loss=1.55 (422.5 examples/sec; 0.303 sec/batch)\n",
      "step 360, loss=1.53 (441.6 examples/sec; 0.290 sec/batch)\n",
      "step 370, loss=1.51 (403.5 examples/sec; 0.317 sec/batch)\n",
      "step 380, loss=1.53 (421.3 examples/sec; 0.304 sec/batch)\n",
      "step 390, loss=1.55 (402.1 examples/sec; 0.318 sec/batch)\n",
      "step 400, loss=1.62 (449.8 examples/sec; 0.285 sec/batch)\n",
      "step 410, loss=1.73 (430.3 examples/sec; 0.297 sec/batch)\n",
      "step 420, loss=1.66 (457.0 examples/sec; 0.280 sec/batch)\n",
      "step 430, loss=1.44 (433.5 examples/sec; 0.295 sec/batch)\n",
      "step 440, loss=1.50 (432.4 examples/sec; 0.296 sec/batch)\n",
      "step 450, loss=1.29 (440.8 examples/sec; 0.290 sec/batch)\n",
      "step 460, loss=1.50 (448.0 examples/sec; 0.286 sec/batch)\n",
      "step 470, loss=1.58 (395.4 examples/sec; 0.324 sec/batch)\n",
      "step 480, loss=1.53 (425.3 examples/sec; 0.301 sec/batch)\n",
      "step 490, loss=1.58 (433.6 examples/sec; 0.295 sec/batch)\n",
      "step 500, loss=1.60 (425.5 examples/sec; 0.301 sec/batch)\n",
      "step 510, loss=1.44 (434.9 examples/sec; 0.294 sec/batch)\n",
      "step 520, loss=1.48 (431.2 examples/sec; 0.297 sec/batch)\n",
      "step 530, loss=1.30 (413.1 examples/sec; 0.310 sec/batch)\n",
      "step 540, loss=1.31 (435.1 examples/sec; 0.294 sec/batch)\n",
      "step 550, loss=1.41 (432.2 examples/sec; 0.296 sec/batch)\n",
      "step 560, loss=1.40 (428.9 examples/sec; 0.298 sec/batch)\n",
      "step 570, loss=1.42 (421.3 examples/sec; 0.304 sec/batch)\n",
      "step 580, loss=1.44 (436.9 examples/sec; 0.293 sec/batch)\n",
      "step 590, loss=1.33 (426.4 examples/sec; 0.300 sec/batch)\n",
      "step 600, loss=1.39 (407.4 examples/sec; 0.314 sec/batch)\n",
      "step 610, loss=1.41 (434.1 examples/sec; 0.295 sec/batch)\n",
      "step 620, loss=1.50 (448.4 examples/sec; 0.285 sec/batch)\n",
      "step 630, loss=1.47 (427.5 examples/sec; 0.299 sec/batch)\n",
      "step 640, loss=1.39 (433.3 examples/sec; 0.295 sec/batch)\n",
      "step 650, loss=1.44 (417.8 examples/sec; 0.306 sec/batch)\n",
      "step 660, loss=1.51 (444.7 examples/sec; 0.288 sec/batch)\n",
      "step 670, loss=1.51 (415.8 examples/sec; 0.308 sec/batch)\n",
      "step 680, loss=1.47 (422.3 examples/sec; 0.303 sec/batch)\n",
      "step 690, loss=1.23 (419.4 examples/sec; 0.305 sec/batch)\n",
      "step 700, loss=1.38 (435.9 examples/sec; 0.294 sec/batch)\n",
      "step 710, loss=1.44 (418.7 examples/sec; 0.306 sec/batch)\n",
      "step 720, loss=1.45 (449.3 examples/sec; 0.285 sec/batch)\n",
      "step 730, loss=1.33 (435.1 examples/sec; 0.294 sec/batch)\n",
      "step 740, loss=1.48 (437.7 examples/sec; 0.292 sec/batch)\n",
      "step 750, loss=1.41 (441.7 examples/sec; 0.290 sec/batch)\n",
      "step 760, loss=1.20 (437.6 examples/sec; 0.292 sec/batch)\n",
      "step 770, loss=1.36 (443.8 examples/sec; 0.288 sec/batch)\n",
      "step 780, loss=1.29 (448.1 examples/sec; 0.286 sec/batch)\n",
      "step 790, loss=1.42 (426.2 examples/sec; 0.300 sec/batch)\n",
      "step 800, loss=1.27 (439.4 examples/sec; 0.291 sec/batch)\n",
      "step 810, loss=1.53 (443.3 examples/sec; 0.289 sec/batch)\n",
      "step 820, loss=1.39 (445.8 examples/sec; 0.287 sec/batch)\n",
      "step 830, loss=1.33 (453.9 examples/sec; 0.282 sec/batch)\n",
      "step 840, loss=1.20 (415.5 examples/sec; 0.308 sec/batch)\n",
      "step 850, loss=1.36 (428.7 examples/sec; 0.299 sec/batch)\n",
      "step 860, loss=1.25 (409.3 examples/sec; 0.313 sec/batch)\n",
      "step 870, loss=1.30 (438.0 examples/sec; 0.292 sec/batch)\n",
      "step 880, loss=1.41 (417.6 examples/sec; 0.306 sec/batch)\n",
      "step 890, loss=1.37 (434.0 examples/sec; 0.295 sec/batch)\n",
      "step 900, loss=1.23 (429.9 examples/sec; 0.298 sec/batch)\n",
      "step 910, loss=1.20 (425.7 examples/sec; 0.301 sec/batch)\n",
      "step 920, loss=1.07 (417.9 examples/sec; 0.306 sec/batch)\n",
      "step 930, loss=1.14 (413.5 examples/sec; 0.310 sec/batch)\n",
      "step 940, loss=1.49 (442.8 examples/sec; 0.289 sec/batch)\n",
      "step 950, loss=1.41 (444.5 examples/sec; 0.288 sec/batch)\n",
      "step 960, loss=1.35 (453.7 examples/sec; 0.282 sec/batch)\n",
      "step 970, loss=1.35 (415.2 examples/sec; 0.308 sec/batch)\n",
      "step 980, loss=1.48 (450.4 examples/sec; 0.284 sec/batch)\n",
      "step 990, loss=1.39 (447.5 examples/sec; 0.286 sec/batch)\n",
      "step 1000, loss=1.27 (413.2 examples/sec; 0.310 sec/batch)\n",
      "step 1010, loss=1.17 (441.3 examples/sec; 0.290 sec/batch)\n",
      "step 1020, loss=1.31 (432.0 examples/sec; 0.296 sec/batch)\n",
      "step 1030, loss=1.20 (430.3 examples/sec; 0.297 sec/batch)\n",
      "step 1040, loss=1.47 (419.4 examples/sec; 0.305 sec/batch)\n",
      "step 1050, loss=1.34 (428.0 examples/sec; 0.299 sec/batch)\n",
      "step 1060, loss=1.40 (439.3 examples/sec; 0.291 sec/batch)\n",
      "step 1070, loss=1.35 (434.4 examples/sec; 0.295 sec/batch)\n",
      "step 1080, loss=1.20 (435.7 examples/sec; 0.294 sec/batch)\n",
      "step 1090, loss=1.23 (443.6 examples/sec; 0.289 sec/batch)\n",
      "step 1100, loss=1.35 (449.1 examples/sec; 0.285 sec/batch)\n",
      "step 1110, loss=1.17 (418.1 examples/sec; 0.306 sec/batch)\n",
      "step 1120, loss=1.26 (464.3 examples/sec; 0.276 sec/batch)\n",
      "step 1130, loss=1.31 (420.1 examples/sec; 0.305 sec/batch)\n",
      "step 1140, loss=1.27 (442.5 examples/sec; 0.289 sec/batch)\n",
      "step 1150, loss=1.17 (423.2 examples/sec; 0.302 sec/batch)\n",
      "step 1160, loss=1.19 (424.0 examples/sec; 0.302 sec/batch)\n",
      "step 1170, loss=1.25 (431.4 examples/sec; 0.297 sec/batch)\n",
      "step 1180, loss=1.39 (456.7 examples/sec; 0.280 sec/batch)\n",
      "step 1190, loss=1.19 (461.4 examples/sec; 0.277 sec/batch)\n",
      "step 1200, loss=1.32 (411.2 examples/sec; 0.311 sec/batch)\n",
      "step 1210, loss=1.37 (409.9 examples/sec; 0.312 sec/batch)\n",
      "step 1220, loss=1.23 (468.5 examples/sec; 0.273 sec/batch)\n",
      "step 1230, loss=1.23 (410.1 examples/sec; 0.312 sec/batch)\n",
      "step 1240, loss=1.00 (405.7 examples/sec; 0.316 sec/batch)\n",
      "step 1250, loss=1.25 (431.9 examples/sec; 0.296 sec/batch)\n",
      "step 1260, loss=1.27 (445.1 examples/sec; 0.288 sec/batch)\n",
      "step 1270, loss=1.12 (429.6 examples/sec; 0.298 sec/batch)\n",
      "step 1280, loss=1.27 (451.8 examples/sec; 0.283 sec/batch)\n",
      "step 1290, loss=1.29 (439.7 examples/sec; 0.291 sec/batch)\n",
      "step 1300, loss=1.15 (414.5 examples/sec; 0.309 sec/batch)\n",
      "step 1310, loss=1.13 (457.3 examples/sec; 0.280 sec/batch)\n",
      "step 1320, loss=1.29 (434.2 examples/sec; 0.295 sec/batch)\n",
      "step 1330, loss=1.08 (438.6 examples/sec; 0.292 sec/batch)\n",
      "step 1340, loss=1.31 (412.0 examples/sec; 0.311 sec/batch)\n",
      "step 1350, loss=1.26 (425.3 examples/sec; 0.301 sec/batch)\n",
      "step 1360, loss=1.19 (404.2 examples/sec; 0.317 sec/batch)\n",
      "step 1370, loss=1.18 (426.1 examples/sec; 0.300 sec/batch)\n",
      "step 1380, loss=1.27 (431.2 examples/sec; 0.297 sec/batch)\n",
      "step 1390, loss=1.19 (414.6 examples/sec; 0.309 sec/batch)\n",
      "step 1400, loss=1.17 (433.0 examples/sec; 0.296 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1410, loss=1.06 (465.8 examples/sec; 0.275 sec/batch)\n",
      "step 1420, loss=1.22 (422.7 examples/sec; 0.303 sec/batch)\n",
      "step 1430, loss=1.21 (401.1 examples/sec; 0.319 sec/batch)\n",
      "step 1440, loss=1.21 (418.3 examples/sec; 0.306 sec/batch)\n",
      "step 1450, loss=1.26 (427.7 examples/sec; 0.299 sec/batch)\n",
      "step 1460, loss=1.18 (453.3 examples/sec; 0.282 sec/batch)\n",
      "step 1470, loss=0.98 (434.6 examples/sec; 0.295 sec/batch)\n",
      "step 1480, loss=1.25 (421.8 examples/sec; 0.303 sec/batch)\n",
      "step 1490, loss=1.24 (444.1 examples/sec; 0.288 sec/batch)\n",
      "step 1500, loss=1.15 (424.8 examples/sec; 0.301 sec/batch)\n",
      "step 1510, loss=1.12 (446.5 examples/sec; 0.287 sec/batch)\n",
      "step 1520, loss=1.23 (442.2 examples/sec; 0.289 sec/batch)\n",
      "step 1530, loss=1.19 (423.3 examples/sec; 0.302 sec/batch)\n",
      "step 1540, loss=1.21 (428.4 examples/sec; 0.299 sec/batch)\n",
      "step 1550, loss=1.07 (432.9 examples/sec; 0.296 sec/batch)\n",
      "step 1560, loss=1.09 (429.4 examples/sec; 0.298 sec/batch)\n",
      "step 1570, loss=1.10 (413.2 examples/sec; 0.310 sec/batch)\n",
      "step 1580, loss=1.12 (441.0 examples/sec; 0.290 sec/batch)\n",
      "step 1590, loss=1.31 (439.6 examples/sec; 0.291 sec/batch)\n",
      "step 1600, loss=1.24 (445.2 examples/sec; 0.288 sec/batch)\n",
      "step 1610, loss=0.97 (422.4 examples/sec; 0.303 sec/batch)\n",
      "step 1620, loss=1.05 (418.3 examples/sec; 0.306 sec/batch)\n",
      "step 1630, loss=1.22 (440.9 examples/sec; 0.290 sec/batch)\n",
      "step 1640, loss=1.14 (430.4 examples/sec; 0.297 sec/batch)\n",
      "step 1650, loss=1.27 (455.8 examples/sec; 0.281 sec/batch)\n",
      "step 1660, loss=1.18 (421.4 examples/sec; 0.304 sec/batch)\n",
      "step 1670, loss=1.34 (419.9 examples/sec; 0.305 sec/batch)\n",
      "step 1680, loss=1.23 (426.3 examples/sec; 0.300 sec/batch)\n",
      "step 1690, loss=1.09 (426.2 examples/sec; 0.300 sec/batch)\n",
      "step 1700, loss=1.29 (406.5 examples/sec; 0.315 sec/batch)\n",
      "step 1710, loss=1.03 (410.8 examples/sec; 0.312 sec/batch)\n",
      "step 1720, loss=1.12 (408.2 examples/sec; 0.314 sec/batch)\n",
      "step 1730, loss=1.26 (406.8 examples/sec; 0.315 sec/batch)\n",
      "step 1740, loss=0.92 (404.3 examples/sec; 0.317 sec/batch)\n",
      "step 1750, loss=1.12 (455.6 examples/sec; 0.281 sec/batch)\n",
      "step 1760, loss=1.32 (434.8 examples/sec; 0.294 sec/batch)\n",
      "step 1770, loss=1.02 (429.7 examples/sec; 0.298 sec/batch)\n",
      "step 1780, loss=1.22 (426.3 examples/sec; 0.300 sec/batch)\n",
      "step 1790, loss=1.11 (420.2 examples/sec; 0.305 sec/batch)\n",
      "step 1800, loss=1.02 (428.5 examples/sec; 0.299 sec/batch)\n",
      "step 1810, loss=1.23 (423.0 examples/sec; 0.303 sec/batch)\n",
      "step 1820, loss=1.04 (410.2 examples/sec; 0.312 sec/batch)\n",
      "step 1830, loss=1.13 (434.8 examples/sec; 0.294 sec/batch)\n",
      "step 1840, loss=0.99 (420.5 examples/sec; 0.304 sec/batch)\n",
      "step 1850, loss=1.14 (419.2 examples/sec; 0.305 sec/batch)\n",
      "step 1860, loss=1.22 (415.1 examples/sec; 0.308 sec/batch)\n",
      "step 1870, loss=1.13 (460.9 examples/sec; 0.278 sec/batch)\n",
      "step 1880, loss=1.07 (454.0 examples/sec; 0.282 sec/batch)\n",
      "step 1890, loss=1.02 (417.3 examples/sec; 0.307 sec/batch)\n",
      "step 1900, loss=1.03 (429.5 examples/sec; 0.298 sec/batch)\n",
      "step 1910, loss=1.05 (441.4 examples/sec; 0.290 sec/batch)\n",
      "step 1920, loss=1.11 (454.7 examples/sec; 0.281 sec/batch)\n",
      "step 1930, loss=1.20 (421.3 examples/sec; 0.304 sec/batch)\n",
      "step 1940, loss=1.09 (425.1 examples/sec; 0.301 sec/batch)\n",
      "step 1950, loss=0.97 (439.0 examples/sec; 0.292 sec/batch)\n",
      "step 1960, loss=1.25 (436.4 examples/sec; 0.293 sec/batch)\n",
      "step 1970, loss=1.29 (425.6 examples/sec; 0.301 sec/batch)\n",
      "step 1980, loss=1.13 (447.1 examples/sec; 0.286 sec/batch)\n",
      "step 1990, loss=1.15 (420.3 examples/sec; 0.305 sec/batch)\n",
      "step 2000, loss=1.15 (432.2 examples/sec; 0.296 sec/batch)\n",
      "step 2010, loss=1.07 (421.5 examples/sec; 0.304 sec/batch)\n",
      "step 2020, loss=1.16 (423.5 examples/sec; 0.302 sec/batch)\n",
      "step 2030, loss=1.19 (432.2 examples/sec; 0.296 sec/batch)\n",
      "step 2040, loss=1.26 (454.9 examples/sec; 0.281 sec/batch)\n",
      "step 2050, loss=1.07 (424.7 examples/sec; 0.301 sec/batch)\n",
      "step 2060, loss=1.13 (437.7 examples/sec; 0.292 sec/batch)\n",
      "step 2070, loss=1.05 (439.0 examples/sec; 0.292 sec/batch)\n",
      "step 2080, loss=0.85 (449.9 examples/sec; 0.285 sec/batch)\n",
      "step 2090, loss=1.05 (437.0 examples/sec; 0.293 sec/batch)\n",
      "step 2100, loss=1.20 (401.6 examples/sec; 0.319 sec/batch)\n",
      "step 2110, loss=1.16 (445.8 examples/sec; 0.287 sec/batch)\n",
      "step 2120, loss=1.16 (428.5 examples/sec; 0.299 sec/batch)\n",
      "step 2130, loss=1.15 (421.3 examples/sec; 0.304 sec/batch)\n",
      "step 2140, loss=1.36 (427.2 examples/sec; 0.300 sec/batch)\n",
      "step 2150, loss=1.31 (450.3 examples/sec; 0.284 sec/batch)\n",
      "step 2160, loss=1.18 (433.3 examples/sec; 0.295 sec/batch)\n",
      "step 2170, loss=0.99 (450.0 examples/sec; 0.284 sec/batch)\n",
      "step 2180, loss=1.14 (418.5 examples/sec; 0.306 sec/batch)\n",
      "step 2190, loss=1.19 (459.4 examples/sec; 0.279 sec/batch)\n",
      "step 2200, loss=1.12 (426.1 examples/sec; 0.300 sec/batch)\n",
      "step 2210, loss=1.13 (425.9 examples/sec; 0.301 sec/batch)\n",
      "step 2220, loss=1.09 (399.9 examples/sec; 0.320 sec/batch)\n",
      "step 2230, loss=0.97 (444.5 examples/sec; 0.288 sec/batch)\n",
      "step 2240, loss=0.99 (444.9 examples/sec; 0.288 sec/batch)\n",
      "step 2250, loss=1.04 (433.6 examples/sec; 0.295 sec/batch)\n",
      "step 2260, loss=1.07 (360.1 examples/sec; 0.355 sec/batch)\n",
      "step 2270, loss=1.03 (423.3 examples/sec; 0.302 sec/batch)\n",
      "step 2280, loss=1.23 (430.5 examples/sec; 0.297 sec/batch)\n",
      "step 2290, loss=1.14 (444.9 examples/sec; 0.288 sec/batch)\n",
      "step 2300, loss=1.10 (425.0 examples/sec; 0.301 sec/batch)\n",
      "step 2310, loss=1.19 (414.6 examples/sec; 0.309 sec/batch)\n",
      "step 2320, loss=0.93 (461.8 examples/sec; 0.277 sec/batch)\n",
      "step 2330, loss=1.01 (430.7 examples/sec; 0.297 sec/batch)\n",
      "step 2340, loss=1.18 (434.3 examples/sec; 0.295 sec/batch)\n",
      "step 2350, loss=1.18 (408.4 examples/sec; 0.313 sec/batch)\n",
      "step 2360, loss=1.02 (416.1 examples/sec; 0.308 sec/batch)\n",
      "step 2370, loss=1.08 (428.0 examples/sec; 0.299 sec/batch)\n",
      "step 2380, loss=1.17 (412.3 examples/sec; 0.310 sec/batch)\n",
      "step 2390, loss=1.23 (437.1 examples/sec; 0.293 sec/batch)\n",
      "step 2400, loss=1.03 (433.3 examples/sec; 0.295 sec/batch)\n",
      "step 2410, loss=1.11 (459.0 examples/sec; 0.279 sec/batch)\n",
      "step 2420, loss=1.18 (427.8 examples/sec; 0.299 sec/batch)\n",
      "step 2430, loss=1.18 (414.5 examples/sec; 0.309 sec/batch)\n",
      "step 2440, loss=1.04 (394.6 examples/sec; 0.324 sec/batch)\n",
      "step 2450, loss=0.89 (426.9 examples/sec; 0.300 sec/batch)\n",
      "step 2460, loss=1.07 (403.4 examples/sec; 0.317 sec/batch)\n",
      "step 2470, loss=1.03 (433.0 examples/sec; 0.296 sec/batch)\n",
      "step 2480, loss=1.03 (459.9 examples/sec; 0.278 sec/batch)\n",
      "step 2490, loss=1.21 (427.2 examples/sec; 0.300 sec/batch)\n",
      "step 2500, loss=1.10 (452.3 examples/sec; 0.283 sec/batch)\n",
      "step 2510, loss=0.91 (476.4 examples/sec; 0.269 sec/batch)\n",
      "step 2520, loss=1.21 (399.4 examples/sec; 0.320 sec/batch)\n",
      "step 2530, loss=1.10 (424.6 examples/sec; 0.301 sec/batch)\n",
      "step 2540, loss=1.17 (408.8 examples/sec; 0.313 sec/batch)\n",
      "step 2550, loss=1.12 (434.1 examples/sec; 0.295 sec/batch)\n",
      "step 2560, loss=1.06 (433.5 examples/sec; 0.295 sec/batch)\n",
      "step 2570, loss=1.20 (434.9 examples/sec; 0.294 sec/batch)\n",
      "step 2580, loss=0.95 (435.8 examples/sec; 0.294 sec/batch)\n",
      "step 2590, loss=1.03 (478.3 examples/sec; 0.268 sec/batch)\n",
      "step 2600, loss=1.01 (441.4 examples/sec; 0.290 sec/batch)\n",
      "step 2610, loss=1.10 (447.5 examples/sec; 0.286 sec/batch)\n",
      "step 2620, loss=1.02 (434.7 examples/sec; 0.294 sec/batch)\n",
      "step 2630, loss=0.92 (434.4 examples/sec; 0.295 sec/batch)\n",
      "step 2640, loss=1.16 (442.6 examples/sec; 0.289 sec/batch)\n",
      "step 2650, loss=1.22 (437.4 examples/sec; 0.293 sec/batch)\n",
      "step 2660, loss=1.09 (410.5 examples/sec; 0.312 sec/batch)\n",
      "step 2670, loss=1.19 (431.1 examples/sec; 0.297 sec/batch)\n",
      "step 2680, loss=1.13 (426.0 examples/sec; 0.300 sec/batch)\n",
      "step 2690, loss=1.17 (451.5 examples/sec; 0.284 sec/batch)\n",
      "step 2700, loss=0.90 (428.5 examples/sec; 0.299 sec/batch)\n",
      "step 2710, loss=1.07 (428.7 examples/sec; 0.299 sec/batch)\n",
      "step 2720, loss=1.01 (416.9 examples/sec; 0.307 sec/batch)\n",
      "step 2730, loss=0.98 (451.3 examples/sec; 0.284 sec/batch)\n",
      "step 2740, loss=1.06 (433.4 examples/sec; 0.295 sec/batch)\n",
      "step 2750, loss=0.87 (405.1 examples/sec; 0.316 sec/batch)\n",
      "step 2760, loss=1.07 (428.6 examples/sec; 0.299 sec/batch)\n",
      "step 2770, loss=1.07 (420.6 examples/sec; 0.304 sec/batch)\n",
      "step 2780, loss=1.11 (446.8 examples/sec; 0.286 sec/batch)\n",
      "step 2790, loss=1.05 (425.1 examples/sec; 0.301 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2800, loss=1.13 (439.5 examples/sec; 0.291 sec/batch)\n",
      "step 2810, loss=1.16 (424.5 examples/sec; 0.302 sec/batch)\n",
      "step 2820, loss=1.00 (438.0 examples/sec; 0.292 sec/batch)\n",
      "step 2830, loss=1.14 (414.6 examples/sec; 0.309 sec/batch)\n",
      "step 2840, loss=0.82 (422.5 examples/sec; 0.303 sec/batch)\n",
      "step 2850, loss=1.10 (412.9 examples/sec; 0.310 sec/batch)\n",
      "step 2860, loss=0.97 (485.3 examples/sec; 0.264 sec/batch)\n",
      "step 2870, loss=1.22 (419.6 examples/sec; 0.305 sec/batch)\n",
      "step 2880, loss=0.95 (453.8 examples/sec; 0.282 sec/batch)\n",
      "step 2890, loss=0.98 (432.0 examples/sec; 0.296 sec/batch)\n",
      "step 2900, loss=0.97 (440.7 examples/sec; 0.290 sec/batch)\n",
      "step 2910, loss=0.95 (462.4 examples/sec; 0.277 sec/batch)\n",
      "step 2920, loss=0.96 (421.4 examples/sec; 0.304 sec/batch)\n",
      "step 2930, loss=1.10 (436.6 examples/sec; 0.293 sec/batch)\n",
      "step 2940, loss=1.32 (434.4 examples/sec; 0.295 sec/batch)\n",
      "step 2950, loss=1.21 (427.8 examples/sec; 0.299 sec/batch)\n",
      "step 2960, loss=1.11 (426.9 examples/sec; 0.300 sec/batch)\n",
      "step 2970, loss=1.06 (504.0 examples/sec; 0.254 sec/batch)\n",
      "step 2980, loss=0.78 (450.5 examples/sec; 0.284 sec/batch)\n",
      "step 2990, loss=0.92 (433.6 examples/sec; 0.295 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "for step in range(max_steps):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    _, loss_value = sess.run([train_op, loss], feed_dict={image_holder:image_batch, label_holder:label_batch})\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        examples_per_sec = batch_size / duration\n",
    "        sec_per_batch = float(duration)\n",
    "        \n",
    "        format_str = ('step %d, loss=%.2f (%.1f examples/sec; %.3f sec/batch)')\n",
    "        print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 测试\n",
    "num_examples = 10000\n",
    "import math\n",
    "num_iter = int(math.ceil(num_examples / batch_size))\n",
    "true_count = 0\n",
    "total_sample_count = num_iter * batch_size\n",
    "step = 0\n",
    "while step < num_iter:\n",
    "    image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "    predictions = sess.run([top_k_op], feed_dict={image_holder:image_batch, label_holder:label_batch})\n",
    "    true_count += np.sum(predictions)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision @ 1 = 0.715\n"
     ]
    }
   ],
   "source": [
    "# 打印结果\n",
    "precision = true_count / total_sample_count\n",
    "print('precision @ 1 = %.3f' % precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
